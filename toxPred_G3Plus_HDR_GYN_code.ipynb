{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201e6a12-d294-4e3d-a4dc-f14e6ab5ec5c",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81221ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os, sys, time\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "from warnings import simplefilter\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n",
    "\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216786e8-2cce-4014-9647-3a7b6ec6f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify database file location\n",
    "db_filename = 'filepath'\n",
    "\n",
    "# Determine file location for objects to be saved\n",
    "cat_imputer_filename = 'filepath'\n",
    "num_imputer_filename = 'filepath'\n",
    "scaler_filename = 'filepath'\n",
    "encoder_filename = 'filepath'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff349f9-32d2-463f-bced-e0f330dd1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel file\n",
    "df = pd.read_excel(db_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd42679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features are going to be the X variables and the Toxicity Labels the Y variable\n",
    "X_pre = df.iloc[:,:-1]\n",
    "y = df.loc[:, ['Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Final DB Columns\n",
    "X_pre.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding values\n",
    "encoding_dict = {\n",
    "    'Chemotherapy': {'N': 0, 'Y': 1},\n",
    "    'Charlson': {'Low': 0, 'Medium': 1, 'High': 2, 'VeryHigh': 3},\n",
    "    'KPS': {'Good': 0, 'Normal': 1, 'Bad': 2},\n",
    "    'Ethnicity': {'C': 0, 'NC': 1},\n",
    "    'TypeOfBoost': {'DidNotGet': 0, 'SIB': 1, 'Sequential': 2},\n",
    "    'Applicator': {'Syed': 0, 'TO': 1},\n",
    "    'MRI': {'N': 0, 'Y': 1},\n",
    "    'TumorSite': {'Other': 0, 'Endometrium': 1, 'Cervix': 2},\n",
    "    'Stage': {'I': 0, 'II': 1, 'III': 2, 'IV': 3},\n",
    "    'Histology': {'SCC': 0, 'Non - SCC': 1},\n",
    "}\n",
    "\n",
    "categorical_columns = [\n",
    "    'Chemotherapy', 'Charlson', 'KPS', 'Ethnicity',\n",
    "    'TypeOfBoost', 'Applicator', 'MRI', 'TumorSite',\n",
    "    'Stage', 'Histology'\n",
    "]\n",
    "\n",
    "# Determine the categorical column indexes to be used later\n",
    "categorical_columns_index = list(map( lambda n: X_pre.columns.get_loc(n), categorical_columns) )\n",
    "\n",
    "# Apply the encoding dictionary to the respective columns\n",
    "for col, mapping in encoding_dict.items():\n",
    "    X_pre[col] = X_pre[col].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63e5f50-668e-4b52-b806-9babf99c5ebd",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pre, y, random_state = 0, stratify = y, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a138d45-da1a-4bdf-ac56-765e6f819133",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training set size -> {X_train.shape[0]}')\n",
    "print(f'Testing set size -> {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a75e79-2f65-42b0-93fc-277f6da31963",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of NEGATIVE cases in TRAIN -> {y_train.value_counts()[0]}')\n",
    "print(f'Number of POSITIVE cases in TRAIN -> {y_train.value_counts()[1]}')\n",
    "print(f'Ratio of P:N -> {y_train.value_counts()[1]/y_train.value_counts()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d181068-bb12-49c9-9669-fac42c15c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of NEGATIVE cases in TEST -> {y_test.value_counts()[0]}')\n",
    "print(f'Number of POSITIVE cases in TEST -> {y_test.value_counts()[1]}')\n",
    "print(f'Ratio of P:N -> {y_test.value_counts()[1]/y_test.value_counts()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc019726-644e-45e0-a417-fd9e47573674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CrossValidation algorithm and parameters \n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc874bc5-ffb3-4a3e-ae93-09a9cf8bb40f",
   "metadata": {},
   "source": [
    "# Missing Data Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960c533-d6f8-49d1-baed-f24baecd4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NaN values in all columns before imputation\n",
    "nan_count = X_train.isna().sum()\n",
    "\n",
    "print(\"\\033[1m\" + \"Missing Values for Each Column \\n\" + \"\\033[0m\")\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d35bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNNImputer with 1 neighbor for imputation of categorical columns\n",
    "imputer_categorical = KNNImputer(n_neighbors = 1) \n",
    "\n",
    "# Impute missing values in the categorical columns for TRAIN data\n",
    "X_train[categorical_columns] = imputer_categorical.fit_transform(X_train)[ : , categorical_columns_index]\n",
    "\n",
    "# Save the fitted categorical imputer\n",
    "joblib.dump(imputer_categorical, cat_imputer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNNImputer with 5 neighbors for imputation of numerical columns\n",
    "imputer_numerical = KNNImputer(n_neighbors = 5)\n",
    "\n",
    "# Impute missing values in the numerical columns for TRAIN data\n",
    "X_imputed_train = imputer_numerical.fit_transform(X_train)\n",
    "X_imputed_train_df = pd.DataFrame(X_imputed_train, columns = X_train.columns)\n",
    "\n",
    "# Save the fitted numerical imputer\n",
    "joblib.dump(imputer_numerical, num_imputer_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of X_train to match the index of y_train\n",
    "X_imputed_train_df.index = y_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b43985-64d8-430d-985d-dc729f77d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NaN values in all columns after imputation for verification\n",
    "nan_count = X_imputed_train_df.isna().sum()\n",
    "\n",
    "print(\"\\033[1m\" + \"Missing Values for Each Column \\n\" + \"\\033[0m\")\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b9b2d-8bf9-42e4-8a27-4fd8a6202725",
   "metadata": {},
   "source": [
    "# Unbalanced Assessment Copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unbalanced copies for final model assessment\n",
    "X_imputed_train_final = X_imputed_train_df.copy(deep = True)\n",
    "y_train_final = np.ravel(y_train.copy(deep = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd515f7-769c-4fc1-bd0c-56fc2e1afff4",
   "metadata": {},
   "source": [
    "# Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbbd73e-2af0-4ee1-b26b-8823f3186a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview of positive and negative cases in Training set\n",
    "print(f'Number of NEGATIVE cases in TRAIN -> {y_train.value_counts()[0]}')\n",
    "print(f'Number of POSITIVE cases in TRAIN -> {y_train.value_counts()[1]}')\n",
    "print(f'Ratio of P:N -> {y_train.value_counts()[1]/y_train.value_counts()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the classes for the training dataframe using SVMSMOTE\n",
    "X_imputed_train_df, y_train = SVMSMOTE(sampling_strategy = 1, random_state = 42).fit_resample(X_imputed_train_df, y_train)\n",
    "\n",
    "# Print new numbers for Training set\n",
    "print(f'Number of NEGATIVE cases in TRAIN -> {y_train.value_counts()[0]}')\n",
    "print(f'Number of POSITIVE cases in TRAIN -> {y_train.value_counts()[1]}')\n",
    "print(f'Ratio of P:N -> {y_train.value_counts()[1]/y_train.value_counts()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783cfd5-66b3-46d7-9028-e88ddfbddcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of balanced data frequency\n",
    "plot_values = y_train.value_counts(ascending = True)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(x = ['0', '1'], height = plot_values.array)\n",
    "\n",
    "plt.title('Balanced Distribution of Targets in Train Data using Synthetic Data\\n')\n",
    "plt.xlabel('Toxicity Class')\n",
    "plt.ylabel('n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1403841-0196-458d-9247-6d3230839761",
   "metadata": {},
   "source": [
    "# StandardScaler and TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759690bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of the numerical features using a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numerical_columns = ['BMI', 'AgeAtCompletion', 'TreatmentDays', 'V100', 'D50', 'D90', 'D98', 'D0.1Bladder', 'D1Bladder',\n",
    "    'D2Bladder', 'D0.1SmallBowel', 'D1SmallBowel', 'D2SmallBowel',\n",
    "    'D0.1Sigmoid', 'D1Sigmoid', 'D2Sigmoid','D0.1Rectum', 'D1Rectum', 'D2Rectum',\n",
    "    'Size', 'HRCTVolume', 'FollowUp']\n",
    "\n",
    "categorical_columns = ['Chemotherapy', 'Charlson', 'KPS', 'Ethnicity', 'TypeOfBoost', 'Applicator', 'MRI', \n",
    "    'TumorSite', 'Stage', 'Histology']\n",
    "\n",
    "X_imputed_train_df[numerical_columns] = scaler.fit_transform(X_imputed_train_df[numerical_columns])\n",
    "\n",
    "# Save the fitted StandardScaler using joblib\n",
    "joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18016bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TargetEncoder\n",
    "target_encoder = TargetEncoder(cols = categorical_columns)\n",
    "\n",
    "# Fit and transform the categorical variables\n",
    "X_imputed_train_df[categorical_columns] = target_encoder.fit_transform(X_imputed_train_df[categorical_columns], y_train)\n",
    "\n",
    "# Save the fitted TargetEncoder\n",
    "joblib.dump(target_encoder, encoder_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b2ec2-f48c-437b-b77b-f7d4db20bfc3",
   "metadata": {},
   "source": [
    "# Drop Colinear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find co-linear features using Pearson Correlation Coefficient\n",
    "corr_matrix = X_imputed_train_df.corr().abs()\n",
    "\n",
    "# Pick up the upper triangle of our co-linearity matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find features with correlation greater than 0.85 and store them in a List\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n",
    "\n",
    "# Print the variables that are co-linear\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6228e-e226-4564-ac80-b7e0182f36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Print the co-lineariaty values for the variable you are planning to drop to see which variables it is co-linear with\n",
    "print(corr_matrix[['D98']]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2011df9-2195-426d-9a2d-367095ce3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which co-linear variables are appropiate to drop\n",
    "to_drop = ['D98','D1Bladder','D0.1SmallBowel', 'D1SmallBowel','D1Sigmoid', 'D0.1Rectum', 'D1Rectum']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fd6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features for TRAINING set\n",
    "X_imputed_train_df = X_imputed_train_df.drop(to_drop, axis=1, inplace=False)\n",
    "\n",
    "# Print Final DB Columns\n",
    "X_imputed_train_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the variance for remaining columns. Variance close to 0 means values are almost constant and may be considered to be dropped.\n",
    "X_imputed_train_df.var(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e558f951-efb0-4d55-b5f4-ac76c7e04668",
   "metadata": {},
   "source": [
    "# Load external instances (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfeb70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all fitted transformers - if needed\n",
    "imputer_categorical = joblib.load(cat_imputer_filename)\n",
    "\n",
    "imputer_numerical = joblib.load(num_imputer_filename)\n",
    "\n",
    "scaler = joblib.load(scaler_filename)\n",
    "\n",
    "target_encoder = joblib.load(encoder_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0a94d4-022f-4f8f-85b6-8723c1b0ba3c",
   "metadata": {},
   "source": [
    "# Preparing datasets for final fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e738de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the numerical columns in the assesment training data (unbalanced)\n",
    "X_imputed_train_final[numerical_columns] = scaler.transform(X_imputed_train_final[numerical_columns])\n",
    "\n",
    "# Apply the TargetEncoder to the categorical columns in the assesment training data (unbalanced)\n",
    "X_imputed_train_final[categorical_columns] = target_encoder.transform(X_imputed_train_final[categorical_columns])\n",
    "\n",
    "# Drop features for the assesment training data (unbalanced)\n",
    "X_imputed_train_final = X_imputed_train_final.drop(to_drop, axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc204227-c370-42ca-b9e7-e1642afe4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data with the previously fitted transformers \n",
    "X_test[categorical_columns] = imputer_categorical.transform(X_test)[ : , categorical_columns_index]\n",
    "X_imputed_test = imputer_numerical.transform(X_test)\n",
    "\n",
    "X_imputed_test_df = pd.DataFrame(X_imputed_test, columns = X_test.columns)\n",
    "X_imputed_test_df.index = y_test.index\n",
    "\n",
    "X_imputed_test_df[numerical_columns] = scaler.transform(X_imputed_test_df[numerical_columns])\n",
    "X_imputed_test_df[categorical_columns] = target_encoder.transform(X_imputed_test_df[categorical_columns])\n",
    "\n",
    "# Drop features for the Testing data\n",
    "X_imputed_test_df = X_imputed_test_df.drop(to_drop, axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NaN values in final datasets\n",
    "nan_count_assesment = X_imputed_train_final.isna().sum()\n",
    "print(\"\\033[1m\" + \"Missing Values for Each Column in Final Training Set\\n\" + \"\\033[0m\")\n",
    "print(nan_count_assesment)\n",
    "\n",
    "nan_count = X_imputed_test_df.isna().sum()\n",
    "print(\"\\n\\033[1m\" + \"Missing Values for Each Column in Testing Set\\n\" + \"\\033[0m\")\n",
    "print(nan_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the format of our Y label so that it is compatible with the Sequential Feature Selection method.\n",
    "labels_test = np.ravel(y_test)\n",
    "labels_train = np.ravel(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ded2c-c4b8-4ff5-bbe6-8eeb9edd14ac",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba18a-e7c1-470c-8ec2-c58a824e9b82",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185df9e6-d4ae-4e75-9e91-80fd822241af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "lr1 = LogisticRegression(random_state = 42, max_iter = 250)\n",
    "lr2 = LogisticRegression(random_state = 42, max_iter = 250)\n",
    "\n",
    "# Sequential Feature Selection\n",
    "sfsLR = SFS(estimator = lr1, \n",
    "    k_features = 'best',\n",
    "    forward = True, \n",
    "    floating = False, \n",
    "    scoring = 'f1',\n",
    "    cv = sss)\n",
    "\n",
    "# Pipeline\n",
    "pipe_lr = Pipeline([\n",
    "    ('lr2', lr2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "param_range_fl = [1.0, 0.7, 0.5, 0.3, 0.1]\n",
    "\n",
    "lr_param_grid = [{'lr2__penalty': ['l1', 'l2'],\n",
    "    'lr2__C': param_range_fl,\n",
    "    'lr2__solver': ['liblinear','lbfgs']}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "lr_grid_search = GridSearchCV(estimator = pipe_lr,\n",
    "    param_grid = lr_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e144e34-3855-4ca3-aaa4-83c4aab4851f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52139104-286d-4c25-b4fa-5fedcef61242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "rf2 = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# SFS placeholder - needed for \"zip\" function used to calculate metrics afterwards\n",
    "sfsRF = 'placeholder'\n",
    "\n",
    "# Pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('rf2', rf2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "param_range_rf = [5, 6, 7, 8, 9, 10]\n",
    "param_estimators_rf = [10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "param_max_features = ['sqrt', 'log2']\n",
    "\n",
    "rf_param_grid = [{'rf2__min_samples_leaf': param_range_rf,\n",
    "        'rf2__n_estimators': param_estimators_rf,\n",
    "        'rf2__min_samples_split': param_range_rf,\n",
    "        'rf2__max_features': param_max_features}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "rf_grid_search = GridSearchCV(estimator = pipe_rf,     \n",
    "    param_grid = rf_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6702539-6c75-40f9-ba4b-dd8a02495adc",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a3c1ea-e580-432e-afa0-d4233fa91865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "knn1 = KNeighborsClassifier()\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "# Sequential Feature Selection\n",
    "sfsKNN = SFS(estimator = knn1, \n",
    "    k_features = 'best',\n",
    "    forward = True, \n",
    "    floating = False, \n",
    "    scoring = 'f1',\n",
    "    cv = sss)\n",
    "\n",
    "# Pipeline\n",
    "pipe_knn = Pipeline([\n",
    "    ('knn2', knn2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "param_range_knn = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "knn_param_grid = [{'knn2__n_neighbors': param_range_knn}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "knn_grid_search = GridSearchCV(estimator = pipe_knn,\n",
    "    param_grid = knn_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d341a82-ae15-418f-ac68-8533c451076d",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc727a4-1c88-4e69-8c91-28c6f350b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "svm1 = SVC(random_state = 42, probability = True)\n",
    "svm2 = SVC(random_state = 42, probability = True)\n",
    "\n",
    "# Sequential Feature Selection\n",
    "sfsSVM = SFS(estimator = svm1, \n",
    "    k_features = 'best',\n",
    "    forward = True, \n",
    "    floating = False, \n",
    "    scoring = 'f1',\n",
    "    cv = sss)\n",
    "\n",
    "# Pipeline\n",
    "pipe_svm = Pipeline([\n",
    "    ('svm2',svm2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "param_range_svm = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "svm_param_grid = [{'svm2__kernel': ['linear', 'rbf','poly','sigmoid'],\n",
    "    'svm2__gamma': ['scale', 'auto'], \n",
    "    'svm2__C': param_range_svm}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "svm_grid_search = GridSearchCV(estimator = pipe_svm,\n",
    "    param_grid = svm_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7508436e-0316-4114-bd7e-abc124dc659f",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ede51f-1ad6-4f54-a896-0a680f06c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "gnb1 = GaussianNB()\n",
    "gnb2 = GaussianNB()\n",
    "\n",
    "# Sequential Feature Selection\n",
    "sfsGNB = SFS(estimator = gnb1, \n",
    "    k_features = 'best',\n",
    "    forward = True, \n",
    "    floating = False, \n",
    "    scoring = 'f1',\n",
    "    cv = sss)\n",
    "\n",
    "# Pipeline\n",
    "pipe_gnb = Pipeline([\n",
    "    ('gnb2',gnb2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "var_smoothingNumbers=[5e-08, 1e-09, 5e-09]\n",
    "\n",
    "gnb_param_grid = [{'gnb2__var_smoothing': var_smoothingNumbers}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "gnb_grid_search = GridSearchCV(estimator = pipe_gnb,\n",
    "    param_grid = gnb_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640083d4-0c5c-4a76-ac59-884abaed039c",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd87e2b-7b8e-4b18-b88e-25b2936a9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "mlp2 = MLPClassifier(hidden_layer_sizes = (16,8), random_state = 42) \n",
    "\n",
    "# SFS placeholder - needed for \"zip\" function used to calculate metrics afterwards\n",
    "sfsMLP = 'placeholder'\n",
    "\n",
    "# Pipeline\n",
    "pipe_mlp = Pipeline([\n",
    "    ('mlp2', mlp2)])\n",
    "\n",
    "# Hyperparameters to explore\n",
    "learning_rates = [0.001, .1, .15, .2, .3, .5, 1]\n",
    "MLPalpha=[0.0001, 0.0005, 0.001]\n",
    "MLPbatchsize=[20, 32, 50]\n",
    "MLPtols=[0.00001, 0.0001, 0.0005]\n",
    "\n",
    "mlp_param_grid = [{ 'mlp2__learning_rate': ['constant', 'invscaling','adaptive'],\n",
    "    'mlp2__activation': ['logistic', 'tanh', 'relu'],\n",
    "    'mlp2__solver': ['lbfgs', 'sgd'],\n",
    "    'mlp2__learning_rate_init': learning_rates,\n",
    "    'mlp2__alpha': MLPalpha,\n",
    "    'mlp2__batch_size': MLPbatchsize,\n",
    "    'mlp2__tol': MLPtols}]\n",
    "\n",
    "# Gridsearch will use the parameters defined on param_grids to find the best Hyper-Parameter for the model.\n",
    "mlp_grid_search = GridSearchCV(estimator = pipe_mlp,     \n",
    "    param_grid = mlp_param_grid,\n",
    "    scoring = 'f1',\n",
    "    cv = sss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca50d1-2489-42e2-8b10-8e2565509a64",
   "metadata": {},
   "source": [
    "# GridSearch Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d6c98-4a13-45d4-b854-8daf07748ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models we are exploring\n",
    "grid_dict = {0: 'Logistic Regression', 1: 'Random Forest', 2: 'K-Nearest Neighbors',\n",
    "3: 'Support Vector Machines', 4: 'Gaussian Naive Bayes', 5: 'Multi-Layer Perceptron'}\n",
    "\n",
    "grids = [lr_grid_search, rf_grid_search, knn_grid_search, svm_grid_search, gnb_grid_search, mlp_grid_search]\n",
    "\n",
    "SFSList = [sfsLR, sfsRF, sfsKNN, sfsSVM, sfsGNB, sfsMLP]\n",
    "\n",
    "total_time_start = time.time()\n",
    "\n",
    "j = 0\n",
    "for pipe, sfs in zip(grids, SFSList):\n",
    "    \n",
    "    if sfs == sfsLR or sfs == sfsKNN or sfs == sfsSVM or sfs == sfsGNB:\n",
    "        \n",
    "        model_time_start = time.time()\n",
    "        \n",
    "        # Fit the SFS to the balanced Train data [This will also update the SFSList items]\n",
    "        sfs = sfs.fit(X_imputed_train_df, labels_train)\n",
    "\n",
    "        # Get selected feature indices\n",
    "        selected_feature_indices = list(sfs.k_feature_idx_)\n",
    "\n",
    "        # Create a pandas DataFrame with selected features\n",
    "        sfsFinal = X_imputed_train_df.iloc[:, selected_feature_indices]\n",
    "\n",
    "        # Rename columns to original feature names\n",
    "        sfsFinal.columns = X_imputed_train_df.columns[selected_feature_indices]\n",
    "\n",
    "        # Perform the GridSearch\n",
    "        pipe.fit(sfsFinal, labels_train)\n",
    "\n",
    "        print(f'Finished {grid_dict[j]} -> {(time.time() - model_time_start)/60} m' )\n",
    "        j += 1\n",
    "        \n",
    "    else:\n",
    "\n",
    "        model_time_start = time.time()\n",
    "        \n",
    "        # Perform the GridSearch\n",
    "        pipe.fit(X_imputed_train_df, labels_train)\n",
    "\n",
    "        print(f'Finished {grid_dict[j]} -> {(time.time() - model_time_start)/60} m' )\n",
    "        j += 1\n",
    "\n",
    "print(f'Total time -> {(time.time() - total_time_start)/60} m' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6d816-e8b1-47f9-9b72-e6c8d7c6c242",
   "metadata": {},
   "source": [
    "# Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1473797b-d463-4941-ac5b-df4eb4a1b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best hyperparameters from GridSearch, the final model is fitted for each algorithm\n",
    "for i, (sfsItem, model) in enumerate(zip(SFSList, grids)):\n",
    "    \n",
    "    if sfsItem == sfsLR or sfsItem == sfsKNN or sfsItem == sfsSVM or sfsItem == sfsGNB:\n",
    "       \n",
    "        print(f'----- {grid_dict[i]} -----')\n",
    "        \n",
    "        # Get the indices of selected features \n",
    "        selected_feature_indices = list(sfsItem.k_feature_idx_)\n",
    "        \n",
    "        print(f'Best Hyper Parameters -> {model.best_params_}')\n",
    "        print(f'SFS F1 Score -> {sfsItem.k_score_}')\n",
    "        print(f'SFS Feature Names -> {sfsItem.k_feature_names_}')\n",
    "        print(f'Score for Trained Tuned model -> {model.best_score_}\\n') \n",
    "        \n",
    "        # Create a pandas DataFrame with selected features\n",
    "        X_train_selected = X_imputed_train_final.iloc[:, selected_feature_indices]\n",
    "        X_test_selected = X_imputed_test_df.iloc[:, selected_feature_indices]\n",
    "       \n",
    "        # Calculate various metrics\n",
    "        y_pred_train = model.predict(X_train_selected)\n",
    "        y_pred_test = model.predict(X_test_selected)\n",
    "    \n",
    "        f1_train = f1_score(y_train_final, y_pred_train)\n",
    "        accuracy_train = accuracy_score(y_train_final, y_pred_train)\n",
    "        precision_train = precision_score(y_train_final, y_pred_train)\n",
    "        recall_train = recall_score(y_train_final, y_pred_train)\n",
    "\n",
    "        print(f'Train Data F1 Score: -> {f1_train}')\n",
    "        print(f'Train Data Accuracy: -> {accuracy_train}')\n",
    "        print(f'Train Data Precision: -> {precision_train}')\n",
    "        print(f'Train Data Recall: -> {recall_train}\\n')\n",
    "        \n",
    "        f1_test = f1_score(labels_test, y_pred_test)\n",
    "        accuracy_test = accuracy_score(labels_test, y_pred_test)\n",
    "        precision_test = precision_score(labels_test, y_pred_test)\n",
    "        recall_test = recall_score(labels_test, y_pred_test)\n",
    "\n",
    "        print(f'Test Data F1 Score: -> {f1_test}')\n",
    "        print(f'Test Data Accuracy: -> {accuracy_test}')\n",
    "        print(f'Test Data Precision: -> {precision_test}')\n",
    "        print(f'Test Data Recall: -> {recall_test}\\n')\n",
    "\n",
    "        # Calculate AUC-ROC and AUC-PR scores\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train_final, model.predict_proba(X_train_selected)[:, 1])\n",
    "        auc_roc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "        precision_train, recall_train, _ = precision_recall_curve(y_train_final, model.predict_proba(X_train_selected)[:, 1])\n",
    "        auc_pr_train = auc(recall_train, precision_train)\n",
    "\n",
    "        print(f'Train Data AUC-ROC Score -> {auc_roc_train}')\n",
    "        print(f'Train Data AUC-PR Score- > {auc_pr_train}')\n",
    "        \n",
    "        fpr_test, tpr_test, _ = roc_curve(labels_test, model.predict_proba(X_test_selected)[:, 1])\n",
    "        auc_roc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "        precision_test, recall_test, _ = precision_recall_curve(labels_test, model.predict_proba(X_test_selected)[:, 1])\n",
    "        auc_pr_test = auc(recall_test, precision_test)\n",
    "\n",
    "        print(f'Test Data AUC-ROC Score -> {auc_roc_test}')\n",
    "        print(f'Test Data AUC-PR Score- > {auc_pr_test}')\n",
    "        print('\\n')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(f'----- {grid_dict[i]} -----')\n",
    "        \n",
    "        print(f'Score for Trained Tuned model -> {model.best_score_}\\n') \n",
    "        \n",
    "        # Rename final sets for code consistency\n",
    "        X_train_selected = X_imputed_train_final\n",
    "        X_test_selected = X_imputed_test_df\n",
    "        \n",
    "        # Calculate various metrics\n",
    "        y_pred_train = model.predict(X_train_selected)\n",
    "        y_pred_test = model.predict(X_test_selected)\n",
    "    \n",
    "        f1_train = f1_score(y_train_final, y_pred_train)\n",
    "        accuracy_train = accuracy_score(y_train_final, y_pred_train)\n",
    "        precision_train = precision_score(y_train_final, y_pred_train)\n",
    "        recall_train = recall_score(y_train_final, y_pred_train)\n",
    "\n",
    "        print(f'Train Data F1 Score: -> {f1_train}')\n",
    "        print(f'Train Data Accuracy: -> {accuracy_train}')\n",
    "        print(f'Train Data Precision: -> {precision_train}')\n",
    "        print(f'Train Data Recall: -> {recall_train}\\n')\n",
    "        \n",
    "        f1_test = f1_score(labels_test, y_pred_test)\n",
    "        accuracy_test = accuracy_score(labels_test, y_pred_test)\n",
    "        precision_test = precision_score(labels_test, y_pred_test)\n",
    "        recall_test = recall_score(labels_test, y_pred_test)\n",
    "\n",
    "        print(f'Test Data F1 Score: -> {f1_test}')\n",
    "        print(f'Test Data Accuracy: -> {accuracy_test}')\n",
    "        print(f'Test Data Precision: -> {precision_test}')\n",
    "        print(f'Test Data Recall: -> {recall_test}\\n')\n",
    "\n",
    "        # Calculate AUC-ROC and AUC-PR scores\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train_final, model.predict_proba(X_train_selected)[:, 1])\n",
    "        auc_roc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "        precision_train, recall_train, _ = precision_recall_curve(y_train_final, model.predict_proba(X_train_selected)[:, 1])\n",
    "        auc_pr_train = auc(recall_train, precision_train)\n",
    "\n",
    "        print(f'Train Data AUC-ROC Score -> {auc_roc_train}')\n",
    "        print(f'Train Data AUC-PR Score- > {auc_pr_train}')\n",
    "        \n",
    "        fpr_test, tpr_test, _ = roc_curve(labels_test, model.predict_proba(X_test_selected)[:, 1])\n",
    "        auc_roc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "        precision_test, recall_test, _ = precision_recall_curve(labels_test, model.predict_proba(X_test_selected)[:, 1])\n",
    "        auc_pr_test = auc(recall_test, precision_test)\n",
    "\n",
    "        print(f'Test Data AUC-ROC Score -> {auc_roc_test}')\n",
    "        print(f'Test Data AUC-PR Score- > {auc_pr_test}')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56420e81-7df2-4dfd-b2e1-fac249f78ed4",
   "metadata": {},
   "source": [
    "# Plot Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the selected features for each model\n",
    "X_test_selected_list = []\n",
    "\n",
    "for i, (sfsItem, model) in enumerate(zip(SFSList, grids)):\n",
    "    \n",
    "    if sfsItem == sfsLR or sfsItem == sfsKNN or sfsItem == sfsSVM or sfsItem == sfsGNB:\n",
    "        # Get the indices of selected features from the current SFS object\n",
    "        selected_feature_indices = list(sfsItem.k_feature_idx_)\n",
    "\n",
    "        # Create a pandas DataFrame with selected features\n",
    "        X_test_selected = X_imputed_test_df.iloc[:, selected_feature_indices]\n",
    "\n",
    "        # Append X_test_selected to the list\n",
    "        X_test_selected_list.append(X_test_selected)\n",
    "    else:\n",
    "        # Rename final set for code consistency\n",
    "        X_test_selected = X_imputed_test_df\n",
    "\n",
    "        # Append X_test_selected to the list\n",
    "        X_test_selected_list.append(X_test_selected)\n",
    "\n",
    "# Calculate the \"No Skill\" line based on your data\n",
    "no_skill = len(labels_test[labels_test == 1]) / len(labels_test)\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--', '-.', ':']\n",
    "\n",
    "# Plot the Precision-Recall curves for all models\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (sfsItem, model) in enumerate(zip(SFSList, grids)):\n",
    "    precision, recall, _ = precision_recall_curve(labels_test, model.predict_proba(X_test_selected_list[i])[:, 1])\n",
    "    \n",
    "    # Calculate AUC\n",
    "    auc_pr = auc(recall, precision)\n",
    "    \n",
    "    # Cycle through line styles using modulo operator\n",
    "    linestyle = line_styles[i % len(line_styles)]\n",
    "    \n",
    "    plt.plot(recall, precision, label='Model {} (AUC = {:.2f})'.format(grid_dict[i], auc_pr), linestyle=linestyle)\n",
    "\n",
    "# Plot the \"No Skill\" line\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\n",
    "\n",
    "plt.xlabel('Recall', fontsize='x-large')\n",
    "plt.ylabel('Precision', fontsize='x-large',labelpad=10)\n",
    "plt.title('Precision-Recall Curve for All Models', fontsize=15)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "# Plot formatting\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.496, -0.15), fancybox=True, shadow=True, ncol=2, fontsize='large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6827e5cf-21ed-42cb-81cf-97c40d3eed11",
   "metadata": {},
   "source": [
    "# Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Create a list to store the selected features for each model\n",
    "X_test_selected_list = []\n",
    "\n",
    "for i, (sfsItem, model) in enumerate(zip(SFSList, grids)):\n",
    "    \n",
    "    if sfsItem == sfsLR or sfsItem == sfsKNN or sfsItem == sfsSVM or sfsItem == sfsGNB:\n",
    "        # Get the indices of selected features from the current SFS object\n",
    "        selected_feature_indices = list(sfsItem.k_feature_idx_)\n",
    "\n",
    "        # Create a pandas DataFrame with selected features\n",
    "        X_test_selected = X_imputed_test_df.iloc[:, selected_feature_indices]\n",
    "\n",
    "        # Append X_test_selected to the list\n",
    "        X_test_selected_list.append(X_test_selected)\n",
    "    else:\n",
    "        # Rename final set for code consistency\n",
    "        X_test_selected = X_imputed_test_df\n",
    "\n",
    "        # Append X_test_selected to the list\n",
    "        X_test_selected_list.append(X_test_selected)\n",
    "\n",
    "# Calculate the \"No Skill\" line based on your data\n",
    "no_skill = len(labels_test[labels_test == 1]) / len(labels_test)\n",
    "line_styles = ['-', '--', '-.', ':', '-', '--', '-.', ':']\n",
    "\n",
    "# Plot the ROC curves for all models\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (sfsItem, model) in enumerate(zip(SFSList, grids)):\n",
    "    fpr, tpr, _ = roc_curve(labels_test, model.predict_proba(X_test_selected_list[i])[:, 1])\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Cycle through line styles using modulo operator\n",
    "    linestyle = line_styles[i % len(line_styles)]\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Model {} (AUC = {:.2f})'.format(grid_dict[i], roc_auc), linestyle=linestyle)\n",
    "\n",
    "# Plot the \"No Skill\" line\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='No Skill')\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize='x-large')\n",
    "plt.ylabel('True Positive Rate', fontsize='x-large',labelpad=10)\n",
    "plt.title('ROC Curve for All Models', fontsize=15)\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "# Plot formatting\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.496, -0.15), fancybox=True, shadow=True, ncol=2, fontsize='large')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
